import argparse
import concurrent.futures
import enum
import functools
import json
import logging
import os
import random
import re
import signal
import sys
import tomllib
from collections import OrderedDict
from collections.abc import Callable, Iterable
from contextlib import AbstractContextManager, nullcontext
from copy import copy
from dataclasses import dataclass
from inspect import getmembers, signature
from io import BytesIO, UnsupportedOperation
from math import copysign
from pathlib import Path
from types import FrameType, TracebackType
from typing import TYPE_CHECKING, Any

import numpy as np
import numpy.linalg as npl
import skrample.sampling as sampling
import skrample.scheduling as scheduling
from PIL import Image, ImageDraw, PngImagePlugin
from skrample.common import MergeStrategy, Predictor, predict_epsilon, predict_flow, predict_sample, predict_velocity
from skrample.sampling import HighOrderSampler, SkrampleSampler
from skrample.scheduling import ScheduleModifier, SkrampleSchedule
from tqdm import tqdm


class _NoErrF(logging.Filter):
    def filter(self, record: logging.LogRecord) -> bool | logging.LogRecord:
        return record.levelno < logging.ERROR


LOGQD: logging.Logger = logging.getLogger("quickdif")
LOGQD.addHandler(logging.StreamHandler(sys.stdout))
LOGQD.handlers[0].addFilter(_NoErrF())
LOGQD.handlers[0].setFormatter(logging.Formatter("[%(name)s @ %(lineno)s]: %(msg)s"))
LOGQD.addHandler(logging.StreamHandler(sys.stderr))
LOGQD.handlers[1].setLevel(logging.ERROR)
LOGQD.handlers[1].setFormatter(logging.Formatter("[%(name)s ! %(lineno)s]: %(msg)s"))
LOGQD.setLevel(logging.WARNING)
LOGQD.propagate = False


# Auto-generated by latent_colors.py
# fmt: off
COLS_FLUX = {"black": [-0.5078, -3.6922, -0.0828, -0.1686, -0.3088, -0.0380, -3.7931, 2.5188, -0.6336, -2.7876, -2.1070, -1.9136, -4.1296, 3.3386, 2.6405, 3.1277], "white": [0.0357, 5.3646, -0.0003, 0.5294, 0.1117, -1.2432, 4.8129, -3.1450, 0.6048, 2.2375, 0.5748, 1.6056, 3.1637, -2.3134, -1.6879, -2.8735], "red": [-2.7140, -1.4865, 4.0531, -0.4655, 0.2901, -1.9045, -2.1971, 2.7449, -1.9715, 2.2922, -3.0625, 2.2470, -0.7138, -1.5384, 2.6211, -0.1633], "green": [-0.0527, 0.1779, -3.6351, -1.2371, 1.2649, 1.4261, 0.7553, -0.1153, -1.2477, 1.4677, 1.2485, -1.8668, -1.5345, 4.0122, -2.1648, 0.6987], "blue": [2.2819, -0.8316, 0.1640, 2.4339, -1.8938, 0.2222, -0.1094, -0.9054, 3.5686, -5.1504, -1.2512, 0.2937, -0.5832, -1.3483, 1.5569, 1.7113], "cyan": [2.0892, 1.9905, -4.1953, 0.5869, 0.0342, 1.8687, 2.5570, -2.5583, 2.4889, -1.9973, 2.1053, -1.3774, 0.0569, 1.6881, -2.4823, 0.5310], "magenta": [0.5596, 0.5332, 3.2740, 1.2936, -1.3963, -1.5772, -0.2316, 0.3176, 1.3820, -1.3756, -2.3867, 2.6160, 1.1095, -4.1404, 2.1839, -0.2943], "yellow": [-3.1145, 1.5895, -0.2401, -1.8268, 2.1619, -0.2006, 0.9635, 0.5971, -3.5224, 5.7695, 0.1277, 0.4292, -0.2327, 1.4972, -1.6908, -1.6265]}  # noqa: E501
COLS_FTMSE = {"black": [-0.9453, -2.5903, 1.1441, 1.3093], "white": [2.1722, 1.3788, 0.0189, -1.1168], "red": [1.2663, -0.8831, -1.8309, 0.7889], "green": [0.7963, 0.7862, 2.3592, 1.8182], "blue": [-0.6164, -2.9997, 0.9888, -1.3812], "cyan": [0.6160, -0.1680, 2.9445, -0.0890], "magenta": [0.7240, -1.5543, -1.3407, -1.4365], "yellow": [2.3246, 1.9687, -0.0343, 1.6547]}  # noqa: E501
COLS_SD3 = {"black": [0.3509, -0.8920, -2.2919, -3.0386, 1.4051, -2.2471, -2.5398, -3.2714, 1.4143, 1.0222, -0.9082, -0.4731, -0.9010, 0.1464, 1.3631, 2.3277], "white": [1.0282, -0.1147, 0.4672, 2.9856, 1.1010, 1.7535, 1.1014, 0.4585, -1.8682, 0.4737, 2.4779, 1.0348, 2.2727, -2.7473, -2.2652, -2.0885], "red": [-1.2336, -2.0950, 1.1345, -0.1707, 2.9721, -2.8930, 0.8448, -5.1268, -0.5644, -0.5753, -0.3742, -1.1464, -0.8675, -0.4272, -0.2791, 0.6347], "green": [-0.3974, -0.3627, -1.2440, -2.4114, 1.6257, 2.8064, -2.8260, -0.2319, -0.9973, 2.6148, 0.3199, 3.0879, -0.4664, 1.1531, -0.1449, 0.0310], "blue": [3.5085, 0.6137, -2.6202, -0.1157, -0.8172, -2.8193, -1.1915, -0.2080, 2.5434, -0.2385, 0.1244, -2.2176, 1.2519, -3.0925, 0.4874, 1.7917], "cyan": [2.2470, 1.0731, -2.5293, -0.4778, -0.6188, 2.3334, -2.5954, 2.2607, 0.0724, 1.9722, 1.5294, 1.9695, 1.9248, -1.8836, -0.4036, -0.4871], "magenta": [1.7010, -0.5941, 0.0239, 2.5945, 0.4832, -3.6681, 1.8616, -2.2589, 0.9498, -1.7815, 0.6008, -3.1938, 1.2960, -3.0539, -0.8390, 0.5577], "yellow": [-1.6557, -1.6916, 1.7513, -0.2434, 3.1708, 1.9903, -0.2312, -2.4154, -2.7154, 1.3859, 1.0528, 2.5863, -0.2995, 0.3382, -1.3912, -1.2726]}  # noqa: E501
COLS_XL = {"black": [-2.8325, 0.5036, 0.3537, 0.3401], "white": [2.3400, 0.2188, 1.2240, -1.0676], "red": [-2.5746, -2.5922, 1.4629, -1.6051], "green": [-0.4603, 1.8433, 3.5376, 1.1637], "blue": [0.0598, 2.1372, -2.2897, 0.5443], "cyan": [1.6232, 3.4135, 0.6123, 1.0585], "magenta": [-0.1147, -0.6371, -1.5609, -1.1584], "yellow": [-0.8686, -1.3799, 4.3088, -1.0480]}  # noqa: E501
# fmt: on

XYZ_M1 = np.array(
    [
        [0.4124, 0.3576, 0.1805],
        [0.2126, 0.7152, 0.0722],
        [0.0193, 0.1192, 0.9505],
    ]
).T

OKLAB_M1 = np.array(
    [
        [0.8189330101, 0.0329845436, 0.0482003018],
        [0.3618667424, 0.9293118715, 0.2643662691],
        [-0.1288597137, 0.0361456387, 0.6338517070],
    ]
)
OKLAB_M2 = np.array(
    [
        [0.2104542553, 1.9779984951, 0.0259040371],
        [0.7936177850, -2.4285922050, 0.7827717662],
        [-0.0040720468, 0.4505937099, -0.8086757660],
    ]
)


def addenv(k: str, val: str) -> None:
    if k not in os.environ:
        os.environ[k] = val


def get_suffix[T, U](
    string: str,
    separator: str = ":::",
    typing: Callable[[str], T] = str,
    default: U = None,
) -> tuple[str, T | U]:
    split = string.rsplit(separator, 1)
    match len(split):
        case 1:
            return (string, default)
        case 2:
            return split[0], typing(split[1])
        case _:
            msg = f"Unable to parse separators `{separator}` for value {string}"
            raise ValueError(msg)


def oversample[T](population: list[T], k: int) -> list[T]:
    samples = []
    while len(samples) < k:
        samples += random.sample(population, min(len(population), k - len(samples)))
    assert len(samples) == k
    return samples


def splitlist(
    values: list[str],
    separator: str = ":::",
    trim_groups: bool = False,
    trim_items: bool = True,
) -> list[list[str]]:
    """Split a list into sub-lists based on a separator.
    `trim_groups` will disallow empty sub-lists, while `trim_items` will disallow empty strings."""
    results = []
    buf = []
    last = ""
    for v in values:
        if v.strip() == separator:
            if buf or not trim_groups:
                results.append(buf)
            buf = []
        elif not trim_items or v.strip():
            buf.append(v)
        last = v
    if buf or (not trim_groups and last.strip() == separator):
        results.append(buf)
    return results


def roundint(n: int | float, step: int) -> int:
    if n % step >= step / 2:
        return round(n + step - (n % step))
    else:
        return round(n - (n % step))


def spowf(n: float | int, power: int | float) -> float:
    return copysign(abs(n) ** power, n)


def spowf_np(array: np.ndarray, power: int | float | list[int | float]) -> np.ndarray:
    return np.copysign(abs(array) ** power, array)


def lrgb_to_oklab(array: np.ndarray) -> np.ndarray:
    return (spowf_np(array @ (XYZ_M1 @ OKLAB_M1), 1 / 3)) @ OKLAB_M2


def oklab_to_lrgb(array: np.ndarray) -> np.ndarray:
    return spowf_np((array @ npl.inv(OKLAB_M2)), 3) @ npl.inv(XYZ_M1 @ OKLAB_M1)


@functools.cache
def _pexpand_bounds(string: str, body: tuple[str, str]) -> None | tuple[int, int]:
    start = len(string) + 1
    end = 0
    escape = False
    for n, c in enumerate(string):
        if escape:
            escape = False
            continue
        elif c == "\\":
            escape = True
            continue
        elif c == body[0]:
            start = n
        elif c == body[1]:
            end = n
        if end > start:
            return (start, end)
    return None


def _pexpand(prompt: str, body: tuple[str, str] = ("{", "}"), sep: str = "|", single: bool = False) -> list[str]:
    bounds = _pexpand_bounds(prompt, body)
    # Split first body; first close after last open
    if bounds is not None:
        prefix = prompt[: bounds[0]]
        suffix = prompt[bounds[1] + 1 :]
        values = []
        current = ""
        escape = False
        for c in prompt[bounds[0] + 1 : bounds[1]]:
            if escape:
                current += c
                escape = False
                continue
            elif c == "\\":
                escape = True
            if c == sep and not escape:
                values.append(current)
                current = ""
            else:
                current += c
        values.append(current)
        if single:
            values = [random.choice(values)]
        results = [prefix + v + suffix for v in values]
    else:
        results = [prompt]

    # Recurse on unexpanded bodies
    results, iterate = [], results
    for result in iterate:
        if _pexpand_bounds(result, body) is None:
            results.append(result)
        else:
            results += pexpand(result, body, sep, single)

    if single:
        results = [random.choice(results)]

    results[:] = dict.fromkeys(results)

    return [result.replace("\\\\", "\x1a").replace("\\", "").replace("\x1a", "\\") for result in results]


@functools.cache
def _pexpand_cache(*args, **kwargs) -> list[str]:  # noqa: ANN002, ANN003
    return _pexpand(*args, **kwargs)


def pexpand(prompt: str, body: tuple[str, str] = ("{", "}"), sep: str = "|", single: bool = False) -> list[str]:
    if single:
        return _pexpand(prompt, body, sep, single)
    else:
        return _pexpand_cache(prompt, body, sep, single)


@enum.unique
class SamplerSK(enum.StrEnum):
    NONE = enum.auto()
    Diffusers = enum.auto()
    DPM = enum.auto()
    SDPM = enum.auto()
    Adams = enum.auto()
    Euler = enum.auto()
    UniPC = enum.auto()
    UniP = enum.auto()
    SPC = enum.auto()

    @property
    def sampler(self) -> tuple[type[SkrampleSampler] | None, dict[str, Any]]:
        match self:
            case SamplerSK.NONE:
                return None, {}
            case SamplerSK.Diffusers:
                return None, {}
            case SamplerSK.DPM:
                return sampling.DPM, {"add_noise": False}
            case SamplerSK.SDPM:
                return sampling.DPM, {"add_noise": True}
            case SamplerSK.Adams:
                return sampling.Adams, {}
            case SamplerSK.UniPC:
                return sampling.UniPC, {}
            case SamplerSK.UniP:
                return sampling.UniP, {}
            case SamplerSK.Euler:
                return sampling.Euler, {}
            case SamplerSK.SPC:
                return sampling.SPC, {}


@enum.unique
class ScheduleSK(enum.StrEnum):
    Default = enum.auto()
    Linear = enum.auto()
    SigmoidCDF = enum.auto()
    Scaled = enum.auto()
    Uniform = enum.auto()
    ZSNR = enum.auto()

    @property
    def schedule(self) -> tuple[type[SkrampleSchedule] | None, dict[str, Any]]:
        match self:
            case ScheduleSK.Default:
                return None, {}
            case ScheduleSK.Linear:
                return scheduling.Linear, {}
            case ScheduleSK.SigmoidCDF:
                return scheduling.SigmoidCDF, {}
            case ScheduleSK.Scaled:
                return scheduling.Scaled, {"uniform": False}
            case ScheduleSK.Uniform:
                return scheduling.Scaled, {"uniform": True}
            case ScheduleSK.ZSNR:
                return scheduling.ZSNR, {"uniform": True}
        return 0


@enum.unique
class PredictorSK(enum.StrEnum):
    Default = enum.auto()
    Epsilon = enum.auto()
    Flow = enum.auto()
    Sample = enum.auto()
    Velocity = enum.auto()

    @property
    def predictor(self) -> Predictor | None:
        match self:
            case PredictorSK.Default:
                return None
            case PredictorSK.Epsilon:
                return predict_epsilon
            case PredictorSK.Flow:
                return predict_flow
            case PredictorSK.Sample:
                return predict_sample
            case PredictorSK.Velocity:
                return predict_velocity
        return 0


@enum.unique
class ModifierSK(enum.StrEnum):
    NONE = enum.auto()
    Beta = enum.auto()
    FlowShift = enum.auto()
    Exponential = enum.auto()
    Karras = enum.auto()
    Hyper = enum.auto()
    Vyper = enum.auto()
    Hype = enum.auto()
    Vype = enum.auto()

    @property
    def schedule_modifier(self) -> tuple[type[ScheduleModifier], dict[str, Any]] | None:
        match self:
            case ModifierSK.NONE:
                return None
            case ModifierSK.Beta:
                return scheduling.Beta, {}
            case ModifierSK.FlowShift:
                return scheduling.FlowShift, {}
            case ModifierSK.Exponential:
                return scheduling.Exponential, {}
            case ModifierSK.Karras:
                return scheduling.Karras, {}
            case ModifierSK.Hyper:
                return scheduling.Hyper, {"tail": True}
            case ModifierSK.Vyper:
                return scheduling.Hyper, {"tail": True, "scale": -scheduling.Hyper.scale}
            case ModifierSK.Hype:
                return scheduling.Hyper, {"tail": False}
            case ModifierSK.Vype:
                return scheduling.Hyper, {"tail": False, "scale": -scheduling.Hyper.scale}
        return 0

    @classmethod
    def parse_suffix(cls, item: str) -> tuple[type[ScheduleModifier], dict[str, Any]] | None:
        modifier, scale = get_suffix(item, typing=float)
        assert modifier in ModifierSK, f"Modifier {modifier} unknown. Must be one of {tuple(cls)}"
        self = cls(modifier)
        modifier = self.schedule_modifier

        if modifier is None:
            return None

        mod_cls, mod_props = modifier
        if scale is not None:
            if issubclass(mod_cls, scheduling.FlowShift):
                mod_props["shift"] = scale
            elif issubclass(mod_cls, scheduling.Karras | scheduling.Exponential):
                mod_props["rho"] = scale
            elif issubclass(mod_cls, scheduling.Hyper):
                mod_props["scale"] = -scale if self in (ModifierSK.Vype, ModifierSK.Vyper) else scale

        return mod_cls, mod_props


@enum.unique
class NoiseSK(enum.StrEnum):
    Random = enum.auto()
    Brownian = enum.auto()
    BrownianReverse = enum.auto()
    Offset = enum.auto()
    OffsetStatic = enum.auto()
    Pyramid = enum.auto()

    @property
    def noise(self) -> tuple[type["sknoise.TensorNoiseCommon"], "sknoise.TensorNoiseProps | None"]:
        match self:
            case NoiseSK.Random:
                return sknoise.Random, None
            case NoiseSK.Brownian:
                return sknoise.Brownian, None
            case NoiseSK.BrownianReverse:
                return sknoise.Brownian, sknoise.BrownianProps(reverse=True)
            case NoiseSK.Offset:
                return sknoise.Offset, None
            case NoiseSK.OffsetStatic:
                return sknoise.Offset, sknoise.OffsetProps(static=True)
            case NoiseSK.Pyramid:
                return sknoise.Pyramid, None
        return 0


@enum.unique
class DTypeSK(enum.StrEnum):
    Default = enum.auto()
    F64 = enum.auto()
    F32 = enum.auto()
    F16 = enum.auto()
    BF16 = enum.auto()

    @property
    def torch_dtype(self) -> "torch.dtype | None":
        match self:
            case DTypeSK.F64:
                return torch.float64
            case DTypeSK.F32:
                return torch.float32
            case DTypeSK.F16:
                return torch.float16
            case DTypeSK.BF16:
                return torch.bfloat16
            case DTypeSK.Default:
                return None
        return 0


@enum.unique
class Iter(enum.StrEnum):
    Basic = enum.auto()
    Walk = enum.auto()
    Shuffle = enum.auto()


@enum.unique
class Sampler(enum.StrEnum):
    Default = enum.auto()
    Ddim = enum.auto()
    Ddpm = enum.auto()
    Euler = enum.auto()
    EulerK = enum.auto()
    EulerE = enum.auto()
    EulerB = enum.auto()
    EulerF = enum.auto()
    EulerFK = enum.auto()
    EulerFE = enum.auto()
    EulerFB = enum.auto()
    EulerA = enum.auto()
    Dpm = enum.auto()
    DpmK = enum.auto()
    SDpm = enum.auto()
    SDpmK = enum.auto()
    Dpm2 = enum.auto()
    Dpm2K = enum.auto()
    SDpm2 = enum.auto()
    SDpm2K = enum.auto()
    Dpm3 = enum.auto()
    Dpm3K = enum.auto()
    SDpm3 = enum.auto()
    SDpm3K = enum.auto()
    Unipc = enum.auto()
    UnipcK = enum.auto()
    Unipc2 = enum.auto()
    Unipc2K = enum.auto()
    Unipc3 = enum.auto()
    Unipc3K = enum.auto()


@enum.unique
class Spacing(enum.StrEnum):
    Leading = enum.auto()
    Trailing = enum.auto()
    Linspace = enum.auto()


@enum.unique
class DType(enum.StrEnum):
    F16 = enum.auto()
    BF16 = enum.auto()
    F32 = enum.auto()
    F8 = enum.auto()
    F8D = enum.auto()
    F6 = enum.auto()
    I8 = enum.auto()
    I8D = enum.auto()
    I4 = enum.auto()
    I4D = enum.auto()
    U7 = enum.auto()
    U6 = enum.auto()
    U5 = enum.auto()
    U4 = enum.auto()
    U3 = enum.auto()
    U2 = enum.auto()
    U1 = enum.auto()

    @property
    def torch_dtype(self) -> "torch.dtype":
        match self:
            case DType.F16:
                return torch.float16
            case DType.BF16:
                return torch.bfloat16
            case DType.F32:
                return torch.float32
            # Quantization parent types
            # FloatX should use F16 instead
            # https://github.com/pytorch/ao/tree/main/torchao/dtypes/floatx
            # Does not work on AMD but performance absolutely tanks with bfloat16 casting
            # So I'll just leave it in for my 0 Nvidia users.
            case DType.F6:
                return torch.float16
            # Rest need BF16
            case _:
                return torch.bfloat16


@enum.unique
class Offload(enum.StrEnum):
    NONE = enum.auto()  # why no assign to None?
    Model = enum.auto()
    Sequential = enum.auto()


@enum.unique
class NoiseType(enum.StrEnum):
    Cpu16 = enum.auto()
    Cpu16B = enum.auto()
    Cpu32 = enum.auto()
    Acc16 = enum.auto()
    Acc16B = enum.auto()
    Acc32 = enum.auto()

    @property
    def torch_device(self) -> str | None:
        match self:
            case NoiseType.Cpu16 | NoiseType.Cpu16B | NoiseType.Cpu32:
                return "cpu"
            case NoiseType.Acc16 | NoiseType.Acc16B | NoiseType.Acc32:
                return None

    @property
    def torch_dtype(self) -> "torch.dtype":
        match self:
            case NoiseType.Cpu16 | NoiseType.Acc16:
                return torch.float16
            case NoiseType.Cpu16B | NoiseType.Acc16B:
                return torch.bfloat16
            case NoiseType.Cpu32 | NoiseType.Acc32:
                return torch.float32


@enum.unique
class AttentionPatch(enum.StrEnum):
    NONE = enum.auto()
    Flash = enum.auto()
    Sage = enum.auto()


@enum.unique
class SDPB(enum.StrEnum):
    Math = enum.auto()
    Flash = enum.auto()
    Efficient = enum.auto()
    CuDNN = enum.auto()

    @property
    def torch_sdp_backend(self) -> "torch._C._SDPBackend":
        match self:
            case SDPB.Math:
                return SDPBackend.MATH
            case SDPB.Flash:
                return SDPBackend.FLASH_ATTENTION
            case SDPB.Efficient:
                return SDPBackend.EFFICIENT_ATTENTION
            case SDPB.CuDNN:
                return SDPBackend.CUDNN_ATTENTION
        return 0


@enum.unique
class Compile(enum.StrEnum):
    Off = enum.auto()
    On = enum.auto()
    On_Dyn = enum.auto()
    On_Full = enum.auto()
    On_Dyn_Full = enum.auto()
    Max = enum.auto()
    Max_Dyn = enum.auto()
    Max_Full = enum.auto()
    Max_Dyn_Full = enum.auto()
    RO = enum.auto()
    RO_Dyn = enum.auto()
    RO_Full = enum.auto()
    RO_Dyn_Full = enum.auto()

    @property
    def plugin(self) -> "TorchDynamoPlugin | None":
        if self is Compile.Off:
            return None

        result = TorchDynamoPlugin(DynamoBackend.INDUCTOR)

        if self in [
            Compile.On_Dyn,
            Compile.On_Dyn_Full,
            Compile.Max_Dyn,
            Compile.Max_Dyn_Full,
            Compile.RO_Dyn,
            Compile.RO_Dyn_Full,
        ]:
            result.dynamic = True

        if self in [
            Compile.On_Full,
            Compile.On_Dyn_Full,
            Compile.Max_Full,
            Compile.Max_Dyn_Full,
            Compile.RO_Full,
            Compile.RO_Dyn_Full,
        ]:
            result.fullgraph = True

        if self in [
            Compile.Max,
            Compile.Max_Dyn,
            Compile.Max_Full,
            Compile.Max_Dyn_Full,
        ]:
            result.mode = "max-autotune"
        elif self in [
            Compile.RO,
            Compile.RO_Dyn,
            Compile.RO_Full,
            Compile.RO_Dyn_Full,
        ]:
            result.mode = "reduce-overhead"

        return result


@enum.unique
class LatentColor(enum.StrEnum):
    Red = enum.auto()
    Green = enum.auto()
    Blue = enum.auto()
    Cyan = enum.auto()
    Magenta = enum.auto()
    Yellow = enum.auto()
    Black = enum.auto()
    White = enum.auto()


class Resolution:
    def __init__(self, resolution: str | tuple[int, int]) -> None:
        if isinstance(resolution, str):
            self._str = resolution
            m = re.match(r"^ *([\d\.]+) *: *([\d\.]+) *(?:: *(\d+))? *(?:([@^]) *([\d\.]+))? *$", resolution)
            if m:
                hor, ver, rnd, method, mpx = m.groups()
                hor, ver = float(hor), float(ver)
                rnd = 64 if rnd is None else int(rnd)
                mpx = 1.0 if mpx is None else float(mpx)
                if method == "^":
                    mpx = mpx * mpx / 10**6
                self._width = roundint(spowf(hor / ver * mpx * 10**6, 1 / 2), rnd)
                self._height = roundint(spowf(ver / hor * mpx * 10**6, 1 / 2), rnd)
            else:
                m = re.match(r"^ *(\d+) *[x*]? *(\d+)? *$", resolution)
                if m is None:
                    m = re.match(r"^ *(\d+)? *[x*] *(\d+) *$", resolution)
                if m:
                    w, h = m.groups()
                    w = 1024 if w is None else int(w)
                    h = 1024 if h is None else int(h)
                    self._width, self._height = w, h
                else:
                    raise ValueError
        else:
            self._str = None
            self._width, self._height = resolution
        if not (16 <= self.width <= 4096 and 16 <= self.height <= 4096):
            raise ValueError

    @property
    def width(self) -> int:
        return self._width

    @property
    def height(self) -> int:
        return self._height

    @property
    def resolution(self) -> tuple[int, int]:
        return (self.width, self.height)

    def __repr__(self) -> str:
        return str(self.width) + "x" + str(self.height)

    def __str__(self) -> str:
        return self._str if self._str is not None else self.__repr__()


class Grid:
    other_iters: tuple[str, ...] = ("resolution", "lora", "dtype")

    def __init__(self, axes: None | str | tuple[str | None, str | None]) -> None:
        self._x: str | None = None
        self._y: str | None = None
        self._str: str | None = None

        if isinstance(axes, str):
            m = re.match(r"^ *([A-Za-z_]+)? *?([,:]?) *([A-Za-z_]+)? *$", axes)
            if m is None:
                raise ValueError
            assert m.group(1) is not None or m.group(3) is not None
            self._x, self._y = m.group(1), m.group(3)
            self._str = axes
        elif isinstance(axes, tuple):
            self._x, self._y = axes

        params = Parameters()

        if self._x is not None:
            self._x = self._x.casefold()
            if self._x == "none":
                self._x = None
            else:
                assert params.get(self._x).meta or self._x in self.other_iters

        if self._y is not None:
            self._y = self._y.casefold()
            if self._y == "none":
                self._y = None
            else:
                assert params.get(self._y).meta or self._y in self.other_iters

    @property
    def x(self) -> str | None:
        return self._x

    @property
    def y(self) -> str | None:
        return self._y

    @property
    def axes(self) -> tuple[str | None, str | None]:
        return (self.x, self.y)

    def fold(self, images: Iterable[tuple[dict[str, Any], Image.Image]]) -> tuple[list[Image.Image], list[Image.Image]]:
        results: list[Image.Image] = []
        # n, x, y
        grids: list[list[list[tuple[dict[str, Any], Image.Image]]]] = []
        others: list[Image.Image] = []

        if self.x is None and self.y is None:
            return (results, [t[1] for t in images])

        for meta, img in images:
            if self.x is not None:
                if self.x not in meta:
                    others.append(img)
                    continue
            if self.y is not None:
                if self.y not in meta:
                    others.append(img)
                    continue

            # Arrange in grid
            n = 0
            while True:
                if len(grids) < (n + 1):
                    grids.append([[(meta, img)]])
                    break

                # add X
                if self.x is not None:
                    if not any(meta[self.x] == gx[0][0][self.x] for gx in grids[n]):
                        grids[n].append([(meta, img)])
                        break

                # add Y
                if self.y is not None:
                    assert self.x is not None, "Pure vertical grids not implemented"
                    do_break = False
                    for gx in grids[n]:
                        if (
                            # All gx columns unique Y
                            not any(meta[self.y] == gy[0][self.y] for gy in gx)
                            # All gx columns same X
                            and all(meta[self.x] == gy[0][self.x] for gy in gx)
                        ):
                            gx.append((meta, img))
                            do_break = True
                            break
                    if do_break:  # gotta love no outer breaks
                        break

                n += 1

        # Compile results
        fill = (255, 255, 255)
        stroke_fill = (0, 0, 0)
        for g in grids:
            cells_x = cells_y = cell_w = cell_h = 0
            for x in g:
                cells_x += 1
                cells_y = max(cells_y, len(x))
                for y in x:
                    cell_w = max(cell_w, y[1].width)
                    cell_h = max(cell_h, y[1].height)

            font_size = cell_w // 25
            stroke_width = max(1, font_size // 10)
            pad = font_size // 4

            canvas = Image.new("RGB", (cells_x * cell_w, cells_y * cell_h), (0, 0, 0))
            for nx, ix in enumerate(g):
                for ny, iy in enumerate(ix):
                    canvas.paste(
                        iy[1],
                        (nx * cell_w + (cell_w - iy[1].width) // 2, ny * cell_h + (cell_h - iy[1].height) // 2),
                    )
                    # X labels
                    if ny == 0 and self.x is not None:
                        draw = ImageDraw.Draw(canvas)
                        draw.text(
                            (nx * cell_w + pad, ny * cell_h),
                            f"{self.x} : {iy[0][self.x]}",
                            fill=fill,
                            stroke_fill=stroke_fill,
                            font_size=font_size,
                            stroke_width=stroke_width,
                        )
                    # Y labels
                    if nx == 0 and self.y is not None:
                        draw = ImageDraw.Draw(canvas)
                        draw.text(
                            (nx * cell_w + pad, (ny + 1) * cell_h - 5),
                            f"{self.y} : {iy[0][self.y]}",
                            anchor="lb",
                            fill=fill,
                            stroke_fill=stroke_fill,
                            font_size=font_size,
                            stroke_width=stroke_width,
                        )
            results.append(canvas)

        return results, others

    def __repr__(self) -> str:
        return str(self.x) + ", " + str(self.y)

    def __str__(self) -> str:
        return self._str if self._str is not None else self.__repr__()


class QDParam[T: str | bool | int | float | Path | Resolution | Grid]:
    def __init__(
        self,
        name: str,
        typing: type[T],
        value: T | None = None,
        short: str | None = None,
        docs: str | None = None,
        multi: bool = False,
        meta: bool = False,
        positional: bool = False,
    ) -> None:
        self.name = name
        self.typing = typing
        self.docs = docs
        self.short = short
        self.multi = multi
        self.meta = meta
        self.positional = positional

        self.value = value
        self.default = copy(self.value)

    def _cast(self, new: str | T) -> T:
        return new if isinstance(new, self.typing) else self.typing(new)

    @property
    def value(self) -> T | list[T] | None:
        return self._value

    @value.setter
    def value(self, new: str | T | list[T | str] | None) -> None:
        if isinstance(new, list):
            if len(new) == 0:
                new = None
        if new is None:
            self._value = new
        elif isinstance(new, list):
            if self.multi:
                self._value = [self._cast(v) for v in new]
            else:
                msg = f"Refusing to assign list '{new}' to non-multi QDParam '{self.name}'"
                raise ValueError(msg)
        else:
            if self.multi:
                self._value = [self._cast(new)]
            else:
                self._value = self._cast(new)

    @property
    def value_multi(self) -> list[T]:
        if isinstance(self._value, list):
            return self._value
        elif self._value is None:
            return []
        else:
            return [self._value]

    @property
    def value_single(self) -> T:
        if isinstance(self._value, list) or self._value is None:
            raise ValueError(self._value)
        return self._value


class Parameters:
    ### Batching
    model = QDParam(
        "model",
        str,
        short="-m",
        value="stabilityai/stable-diffusion-xl-base-1.0",
        meta=True,
        multi=True,
        docs="Safetensor file or HuggingFace model ID. Append `:::` to denote revision",
    )
    prompt = QDParam("prompt", str, multi=True, meta=True, positional=True, docs="Positive prompt")
    negative = QDParam("negative", str, short="-n", multi=True, meta=True, docs="Negative prompt")
    seed = QDParam("seed", int, short="-e", multi=True, meta=True, docs="Seed for RNG")
    resolution = QDParam(
        "resolution",
        Resolution,
        short="-r",
        multi=True,
        docs="Resolution in either [width]x[height] or aspect_x:aspect_y[:round][@megapixels|^square] formats.",
    )
    steps = QDParam(
        "steps",
        int,
        short="-s",
        value=30,
        multi=True,
        meta=True,
        docs="Amount of denoising steps. Prior/Decoder models this only affects the Prior",
    )
    decoder_steps = QDParam(
        "decoder_steps",
        int,
        short="-ds",
        value=-8,
        multi=True,
        meta=True,
        docs="Amount of denoising steps for the Decoder if applicable",
    )
    guidance = QDParam(
        "guidance",
        float,
        short="-g",
        value=5.0,
        multi=True,
        meta=True,
        docs="CFG/Classier-Free Guidance. "
        "Will guide diffusion more strongly towards the prompts. "
        "High values will produce unnatural images",
    )
    decoder_guidance = QDParam(
        "decoder_guidance",
        float,
        short="-dg",
        multi=True,
        meta=True,
        docs="Guidance for the Decoder stage if applicable",
    )
    rescale = QDParam(
        "rescale",
        float,
        short="-G",
        value=0.0,
        multi=True,
        meta=True,
        docs="Rescale the noise during guidance. "
        "Moderate values may help produce more natural images when using strong guidance",
    )
    pag = QDParam(
        "pag",
        float,
        value=0.0,
        multi=True,
        meta=True,
        docs="Perturbed-Attention Guidance scale",
    )
    denoise = QDParam(
        "denoise",
        float,
        short="-d",
        multi=True,
        meta=True,
        docs="Denoising amount for Img2Img. Higher values will change more",
    )
    noise_type = QDParam(
        "noise_type",
        NoiseType,
        short="-nt",
        value=NoiseType.Cpu32,
        multi=True,
        meta=True,
        docs="Device and precision to source RNG from. "
        "To reproduce seeds from other diffusion programs it may be necessary to change this",
    )
    noise_power = QDParam(
        "noise_power",
        float,
        short="-np",
        multi=True,
        meta=True,
        docs="Multiplier to the initial latent noise if applicable. <1 for smoother, >1 for more details",
    )
    color = QDParam(
        "color",
        LatentColor,
        short="-C",
        value=LatentColor.Black,
        multi=True,
        meta=True,
        docs="Color of initial latent noise if applicable. Currently only for XL and SD-FT-MSE latent spaces",
    )
    color_power = QDParam(
        "color_power",
        float,
        short="-c",
        multi=True,
        meta=True,
        docs="Power/opacity of colored latent noise",
    )
    variance_scale = QDParam(
        "variance_scale",
        int,
        short="-vs",
        value=2,
        multi=True,
        meta=True,
        docs="Amount of 'zones' for variance noise. '2' will make a 2x2 grid or 4 tiles",
    )
    variance_power = QDParam(
        "variance_power",
        float,
        short="-vp",
        multi=True,
        meta=True,
        docs="Power/opacity for variance noise. "
        "Variance noise simply adds randomly generated colored zones to "
        "encourage new compositions on overfitted models",
    )
    power = QDParam(
        "power",
        float,
        multi=True,
        meta=True,
        docs="Simple filter which scales final image values away from gray based on an exponent",
    )
    pixelate = QDParam(
        "pixelate",
        float,
        multi=True,
        meta=True,
        docs="Pixelate image using a divisor. Best used with a pixel art Lora",
    )
    posterize = QDParam(
        "posterize",
        int,
        multi=True,
        meta=True,
        docs="Set amount of colors per channel. Best used with --pixelate",
    )
    sampler = QDParam(
        "sampler",
        Sampler,
        short="-S",
        value=Sampler.Default,
        multi=True,
        meta=True,
        docs="""Sampler to use in denoising. Naming scheme is as follows:
euler/ddim/etc. - Literal names;
k - Use karras sigmas;
s - Use SDE stochastic noise;
a - Use ancestral sampling;
2/3 - Use 2nd/3rd order sampling;
Ex. 'sdpm2k' is equivalent to 'DPM++ 2M SDE Karras'""",
    )
    dtype = QDParam(
        "dtype",
        DType,
        short="-dt",
        value=DType.F16,
        multi=True,
        docs="Data format for inference. Should be left at FP16 unless the device or model does not work properly",
    )
    spacing = QDParam(
        "spacing",
        Spacing,
        value=Spacing.Trailing,
        multi=True,
        meta=True,
        docs="Sampler timestep spacing",
    )
    skrample_sampler = QDParam(
        "skrample_sampler",
        SamplerSK,
        short="-K",
        value=SamplerSK.NONE,
        multi=True,
        meta=True,
        docs="""Sampler from https://github.com/Beinsezii/skrample
If this is customized, the diffusers scheduler --scheduler -S will be ignored
Variants with 's' are stochastic, so seuler -> Euler Ancestral""",
    )
    skrample_schedule = QDParam(
        "skrample_schedule",
        ScheduleSK,
        value=ScheduleSK.Default,
        short="-Ks",
        multi=True,
        meta=True,
        docs="""Noise schedule from https://github.com/Beinsezii/skrample
Use Flow/Linear for flow-matching models like SD3 and Flux, otherwise use Scaled/Uniform.
ZSNR is only for very particular models that explicitly need it, like Terminus or NoobAI-VPred.""",
    )
    skrample_predictor = QDParam(
        "skrample_predictor",
        PredictorSK,
        value=PredictorSK.Default,
        short="-Kp",
        multi=True,
        meta=True,
        docs="""Prediction function from https://github.com/Beinsezii/skrample
This should only need to be set if a model explicitly does not use the default, like Terminus and NoobAI-VPred.""",
    )
    skrample_modifier = QDParam(
        "skrample_modifier",
        str,
        value=ModifierSK.NONE,
        short="-Km",
        multi=True,
        meta=True,
        docs="""Schedule modifiers from https://github.com/Beinsezii/skrample
Affect the ramp of the noise schedule. All modifiers work with all schedules to varying degrees.""",
    )
    skrample_modifier_merge = QDParam(
        "skrample_modifier_merge",
        MergeStrategy,
        value=MergeStrategy.UniqueBefore,
        short="-KM",
        multi=True,
        meta=True,
        docs="Controls how set skrample modifiers are merged to the base schedule modifiers.",
    )
    skrample_noise = QDParam(
        "skrample_noise",
        NoiseSK,
        value=NoiseSK.Random,
        short="-Kn",
        multi=True,
        meta=True,
        docs="""The type of noise to use for stochastic skrample samplers.
Random is the default for quickdif and what is used for diffusers schedulers (-S).
Brownian is what is what some other diffusion applications use for "SDE", such as "DPM++ SDE".
Offset is equivalent to the offset noise algorithm; each channel is offset by a random amount.
Pyramid is a hierarchical algorithm; similar to variance_noise.""",
    )
    skrample_order = QDParam(
        "skrample_order",
        int,
        short="-Ko",
        multi=True,
        meta=True,
        docs="""Solver order for skrample_sampler.
This is equivalent to the number in the sampler name of other diffusion applications.
So -Ko 3 is equivalent to DPM++ 3M, or -S dpm3""",
    )
    skrample_dtype = QDParam(
        "skrample_dtype",
        DTypeSK,
        value=DTypeSK.F64,
        short="-Kdt",
        multi=True,
        meta=True,
        docs="""Skrample samplers support a dtype separate from the model dtype itself.
Most diffusion applications use F32, sometimes labeled 'upcast sampling'.
Performance penalty is typically imperceptible, so it's recommended to leave this at F64""",
    )
    ### Global
    lora = QDParam("lora", str, short="-l", meta=True, multi=True, docs='Apply Loras, ex. "ms_paint.safetensors:::0.6"')
    batch_count = QDParam("batch_count", int, short="-b", value=1, docs="Behavior dependant on 'iter'")
    batch_size = QDParam("batch_size", int, short="-B", value=1, docs="Amount of images to produce in each job")
    iter = QDParam(
        "iter",
        Iter,
        value=Iter.Basic,
        docs="""Controls how jobs are created:
'basic' - Run every combination of parameters 'batch_count' times, incrementing seed each 'batch_count';
'walk' - Run every combination of parameters 'batch_count' times, incrementing seed for every individual job;
'shuffle' - Pick randomly from all given parameters 'batch_count' times""",
    )
    grid = QDParam(
        "grid",
        Grid,
        docs="Compile the images into a final grid using up to two parameters, formatted [X or none] ,: [Y or none]",
    )
    ### System
    output = QDParam(
        "output",
        Path,
        short="-o",
        value=Path("./quickdif_output/"),
        docs="Output directory for images",
    )
    offload = QDParam(
        "offload",
        Offload,
        value=Offload.NONE,
        docs="Set amount of CPU offload. "
        "In most UIs, 'model' is equivalent to --med-vram while 'sequential' is equivalent to --low-vram",
    )
    attn_patch = QDParam(
        "attn-patch",
        AttentionPatch,
        value=AttentionPatch.NONE,
        docs="Patch the SDPA function with a custom external attention processor.",
    )
    sdpb = QDParam("sdpb", SDPB, multi=True, docs="Override the SDP attention backend(s) to use")
    compile = QDParam("compile", Compile, value=Compile.Off, docs="Compile network with torch.compile()")
    tunable = QDParam("tunable", bool, value=False, docs="Enable tunable pytorch operations")
    tile = QDParam(
        "tile",
        bool,
        value=False,
        docs="Tile VAE. Slicing is already used by default so only set tile if creating very large images",
    )
    miopen_autotune = QDParam(
        "miopen_autotune",
        bool,
        value=False,
        docs="""Set whether AMD MIOpen autotuning is enabled.
This only applies if the environment variable `MIOPEN_FIND_MODE` is unset.
For information, see `https://rocmdocs.amd.com/projects/MIOpen/en/latest/how-to/find-and-immediate.html#find-modes`""",
    )
    comment = QDParam("comment", str, meta=True, docs="Add a comment to the image.")

    def pairs(self) -> list[tuple[str, QDParam]]:
        return [(k, v) for k, v in getmembers(self) if isinstance(v, QDParam)]

    def labels(self) -> list[str]:
        return [kv[0] for kv in self.pairs()]

    def params(self) -> list[QDParam]:
        return [kv[1] for kv in self.pairs()]

    def get(self, key: str) -> QDParam:  # not __getitem__ because direct field access should be used when possible
        for k, v in self.pairs():
            if k == key:
                return v
        raise ValueError

    def __contains__(self, key: str) -> bool:
        return key in self.labels()

    def __setattr__(self, _: str, __: None) -> None:
        raise NotImplementedError


def build_parser(parameters: Parameters) -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Quick and easy inference for a variety of Diffusers models. Not all models support all options",
        add_help=False,
    )
    for param in parameters.params():
        if param.positional:
            flags = [param.name]
        else:
            flags = [param.short] if param.short else []
            flags.append("--" + param.name.replace("_", "-"))

        kwargs = {}

        docs = [param.docs]
        if isinstance(param.value, list) and len(param.value) == 1:
            docs += [f'Default "{param.value[0]}"']
        elif param.value is not None:
            docs += [f'Default "{param.value}"']
        docs = ". ".join([h for h in docs if h is not None])

        if docs:
            kwargs["help"] = docs

        if param.typing is bool and param.multi is False:
            kwargs["action"] = argparse.BooleanOptionalAction
        else:
            kwargs["type"] = param.typing

            if issubclass(param.typing, enum.Enum):
                kwargs["choices"] = [e.value for e in param.typing]

            if param.multi:
                kwargs["nargs"] = "*"
            else:
                kwargs["nargs"] = "?"

        parser.add_argument(*flags, **kwargs)

    parser.add_argument("-i", "--input", type=argparse.FileType(mode="rb"), help="Input image")
    parser.add_argument(
        "-I",
        "--include",
        type=argparse.FileType(mode="rb"),
        nargs="*",
        help="Include parameters from a quickdif png, json, or toml",
    )
    parser.add_argument("--json", type=argparse.FileType(mode="a+b"), help="Output settings to JSON")
    # It would be nice to write toml but I don't think its worth a 3rd party lib
    # parser.add_argument("--toml", type=argparse.FileType(mode="a+b"), help="Output settings to TOML")
    parser.add_argument("--print", action="store_true", help="Print out generation params and exit")
    parser.add_argument("-.", action="count", help="End the current multi-value optional input")
    parser.add_argument("-V", default=0, action="count", help="Logging verbosity")
    parser.add_argument("--help", action="help")

    return parser


def parse_cli(parameters: Parameters) -> Image.Image | None:
    args = vars(build_parser(parameters).parse_args())
    LOGQD.setLevel(max(LOGQD.level - args["V"] * 10, 10))

    for ext in "json", "toml":
        default = Path(__file__).parent.joinpath(Path(f"quickdif.{ext}"))
        if default.exists():
            reader = default.open(mode="rb")
            if args["include"] is None:
                args["include"] = [reader]
            else:
                args["include"].insert(0, reader)

    if args["include"]:
        for reader in args["include"]:
            data = reader.read()
            try:
                data = data.decode()
                for f, e in [
                    (tomllib.loads, tomllib.TOMLDecodeError),
                    (json.loads, json.JSONDecodeError),
                ]:
                    try:
                        for k, v in f(data).items():
                            if k in parameters:
                                try:
                                    parameters.get(k).value = v
                                except ValueError:
                                    LOGQD.warning(
                                        f"Config value '{v}' cannot be assigned to parameter "
                                        f"'{parameters.get(k).name}', ignoring"
                                    )
                            else:
                                LOGQD.warning(f'Unknown key in serial config "{k}"')
                        break
                    except e:
                        pass
            except UnicodeDecodeError:
                with Image.open(BytesIO(data)) as meta_image:
                    parameters.resolution.value = "x".join(map(str, meta_image.size))
                    for k, v in getattr(meta_image, "text", {}).items():
                        if k in parameters:
                            if k in ["lora", "skrample_modifier"]:
                                parameters.get(k).value = v.split("\x1f")
                            elif parameters.get(k).meta:
                                try:
                                    parameters.get(k).value = v
                                except ValueError:
                                    LOGQD.warning(
                                        f"Config value '{v}' cannot be assigned to parameter "
                                        f"'{parameters.get(k).name}', ignoring"
                                    )

    for key, val in args.items():
        if (
            key in parameters
            and val is not None
            and not (isinstance(val, list) and len(val) == 0 and parameters.get(key).positional)
        ):
            parameters.get(key).value = val

    args = {k: v for k, v in args.items() if k not in parameters}

    if args.get("json", None) is not None:
        dump = {}
        for k, v in parameters.pairs():
            if v.value != v.default:
                val = v.value
                if isinstance(val, Path) or isinstance(val, Grid):
                    val = str(val)
                elif isinstance(val, list):
                    if all(isinstance(x, Resolution) for x in val):
                        val = [str(val) for val in val]
                dump[k] = v
        s = json.dumps(dump)
        try:
            args["json"].seek(0)
            args["json"].truncate()
        except UnsupportedOperation:
            pass
        args["json"].write(s.encode())
        exit()

    for key in "prompt", "negative":
        if (param := parameters.get(key)).value is not None:
            param.value = [expanded for nested in [pexpand(p) for p in param.value] for expanded in nested]

    for sm in (i for sl in splitlist(parameters.skrample_modifier.value_multi) for i in sl):
        ModifierSK.parse_suffix(sm)

    if args.get("print", False):
        print("\n".join([f"{p.name}: {p.value}" for p in parameters.params()]))  # noqa: T201
        exit()

    if parameters.output.value_single.is_dir():
        pass
    elif not parameters.output.value_single.exists():
        parameters.output.value_single.mkdir()
    else:
        msg = "out must be directory"
        raise ValueError(msg)

    input_image = None
    if args["input"]:
        input_image = Image.open(args["input"]).convert("RGB")

    return input_image


do_miopen_autotune = False
if __name__ == "__main__":
    cli_params = Parameters()
    cli_image = parse_cli(cli_params)
    do_miopen_autotune = cli_params.miopen_autotune.value
    if not cli_params.model.value:
        LOGQD.fatal("No model was provided! Exiting...")
        exit()


# Skip kernel lookups as they're really bad in ROCm 6.2+
if not do_miopen_autotune:
    addenv("MIOPEN_FIND_MODE", "FAST")
# Enables memory efficient attention on GFX11/Navi31
addenv("TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL", "1")


#
# Load Torch and libs that depend on it after the CLI cause it's laggy.
import diffusers
import skrample.pytorch.noise as sknoise
import torch
from accelerate.accelerator import Accelerator
from accelerate.utils.dataclasses import DynamoBackend, TorchDynamoPlugin
from compel import Compel, ReturnedEmbeddingsType
from diffusers.configuration_utils import ConfigMixin
from diffusers.loaders.single_file_utils import SingleFileComponentError
from diffusers.pipelines.auto_pipeline import (
    AUTO_TEXT2IMAGE_PIPELINES_MAPPING,
    AutoPipelineForImage2Image,
    AutoPipelineForText2Image,
)
from diffusers.pipelines.flux.pipeline_flux import FluxPipeline
from diffusers.pipelines.hunyuandit.pipeline_hunyuandit import HunyuanDiTPipeline
from diffusers.pipelines.kolors.pipeline_kolors import KolorsPipeline
from diffusers.pipelines.kolors.pipeline_kolors_img2img import KolorsImg2ImgPipeline
from diffusers.pipelines.lumina.pipeline_lumina import LuminaText2ImgPipeline
from diffusers.pipelines.pag.pipeline_pag_hunyuandit import HunyuanDiTPAGPipeline
from diffusers.pipelines.pag.pipeline_pag_kolors import KolorsPAGPipeline
from diffusers.pipelines.pag.pipeline_pag_pixart_sigma import PixArtSigmaPAGPipeline
from diffusers.pipelines.pag.pipeline_pag_sd import StableDiffusionPAGPipeline
from diffusers.pipelines.pag.pipeline_pag_sd_3 import StableDiffusion3PAGPipeline
from diffusers.pipelines.pag.pipeline_pag_sd_xl import StableDiffusionXLPAGPipeline
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
from diffusers.pipelines.pixart_alpha.pipeline_pixart_alpha import PixArtAlphaPipeline
from diffusers.pipelines.pixart_alpha.pipeline_pixart_sigma import PixArtSigmaPipeline
from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipeline
from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import (
    StableDiffusionImg2ImgPipeline,
)
from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3 import StableDiffusion3Pipeline
from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3_img2img import (
    StableDiffusion3Img2ImgPipeline,
)
from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline
from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl_img2img import (
    StableDiffusionXLImg2ImgPipeline,
)
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler
from diffusers.schedulers.scheduling_euler_ancestral_discrete import EulerAncestralDiscreteScheduler
from diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler
from diffusers.schedulers.scheduling_flow_match_euler_discrete import FlowMatchEulerDiscreteScheduler
from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler
from diffusers.schedulers.scheduling_utils import SchedulerMixin
from skrample.diffusers import SkrampleWrapperScheduler
from torch import Tensor
from torch.nn.attention import SDPBackend
from torchao.quantization import (
    float8_dynamic_activation_float8_weight,
    float8_weight_only,
    fpx_weight_only,
    int4_weight_only,
    int8_dynamic_activation_int4_weight,
    int8_dynamic_activation_int8_weight,
    int8_weight_only,
    quantize_,
    uintx_weight_only,
)
from transformers import CLIPTokenizer, T5EncoderModel

if TYPE_CHECKING:
    from diffusers.loaders.single_file import FromSingleFileMixin

if "SanaPipeline" in dir(diffusers) and "sana" not in AUTO_TEXT2IMAGE_PIPELINES_MAPPING:
    AUTO_TEXT2IMAGE_PIPELINES_MAPPING |= {
        "sana": getattr(diffusers, "SanaPipeline"),
        "sana-pag": getattr(diffusers, "SanaPAGPipeline"),
    }

if "QwenImagePipeline" in dir(diffusers) and "qwen" not in AUTO_TEXT2IMAGE_PIPELINES_MAPPING:
    AUTO_TEXT2IMAGE_PIPELINES_MAPPING["qwen"] = getattr(diffusers, "QwenImagePipeline")

if hasattr(torch.backends.cuda, "allow_fp16_bf16_reduction_math_sdp"):
    torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)

# Patch pipes in while PRs await merge
diffusers.pipelines.auto_pipeline.AUTO_TEXT2IMAGE_PIPELINES_MAPPING = OrderedDict(
    [(k, v) for k, v in diffusers.pipelines.auto_pipeline.AUTO_TEXT2IMAGE_PIPELINES_MAPPING.items()]
    + [("lumina", LuminaText2ImgPipeline)]
)
diffusers.pipelines.auto_pipeline.SUPPORTED_TASKS_MAPPINGS[0] = (
    diffusers.pipelines.auto_pipeline.AUTO_TEXT2IMAGE_PIPELINES_MAPPING
)


@dataclass
class PipeRef:
    name: str | None = None
    loras: str | None = None
    dtype: DType | None = None
    pipe: DiffusionPipeline | None = None


# elegent solution from <https://stackoverflow.com/questions/842557/>
class SmartSigint(AbstractContextManager):
    def __init__(self, num: int = 1, job_name: str | None = None) -> None:
        self.num: int = num
        self.job_name: str = job_name if job_name is not None else "current job"

    def __enter__(self) -> None:
        self.count: int = 0
        self.received = None
        self.old_handler: signal._HANDLER = signal.signal(signal.SIGINT, self.handler)

    def handler(self, sig: int, frame: FrameType | None) -> None:
        self.received = (sig, frame)
        if self.count >= self.num:
            LOGQD.info(f"\nSIGINT received {self.count + 1} times, forcibly aborting {self.job_name}")
            self.terminate()
        else:
            LOGQD.info(
                f"\nSIGINT received, waiting for {self.job_name} to complete before exiting."
                f"\nRequire {self.num - self.count} more to abort."
            )
        self.count += 1

    def terminate(self) -> None:
        signal.signal(signal.SIGINT, self.old_handler)
        if self.received and callable(self.old_handler):
            self.old_handler(*self.received)

    def __exit__(
        self, exc_type: type[BaseException] | None, exc_value: BaseException | None, traceback: TracebackType | None, /
    ) -> None:
        self.terminate()


# BOTTOM LEVEL


def is_xl_vae(pipe: DiffusionPipeline) -> bool:
    # TODO (beinsezii): agnostic detection?
    return isinstance(
        pipe,
        HunyuanDiTPAGPipeline
        | HunyuanDiTPipeline
        | KolorsImg2ImgPipeline
        | KolorsPAGPipeline
        | KolorsPipeline
        | LuminaText2ImgPipeline
        | PixArtSigmaPAGPipeline
        | PixArtSigmaPipeline
        | StableDiffusionXLImg2ImgPipeline
        | StableDiffusionXLPAGPipeline
        | StableDiffusionXLPipeline,
    )


def is_sd_vae(pipe: DiffusionPipeline) -> bool:
    return isinstance(
        pipe,
        PixArtAlphaPipeline | StableDiffusionImg2ImgPipeline | StableDiffusionPAGPipeline | StableDiffusionPipeline,
    )


def is_sd3_vae(pipe: DiffusionPipeline) -> bool:
    return isinstance(pipe, StableDiffusion3Pipeline | StableDiffusion3Img2ImgPipeline | StableDiffusion3PAGPipeline)


def is_flux_vae(pipe: DiffusionPipeline) -> bool:
    return isinstance(pipe, FluxPipeline)


def _patch_sdpa(
    patch_func: Callable[[Tensor, Tensor, Tensor, Tensor | None, float, bool, float | None], Tensor],
) -> None:
    """(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None)"""

    torch_sdpa = torch.nn.functional.scaled_dot_product_attention

    def sdpa_hijack_flash(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        attn_mask: Tensor | None = None,
        dropout_p: float = 0.0,
        is_causal: bool = False,
        scale: float | None = None,
    ) -> Tensor:
        try:
            return patch_func(query, key, value, attn_mask, dropout_p, is_causal, scale)
        except BaseException:  # noqa: BLE001
            hidden_states = torch_sdpa(
                query=query,
                key=key,
                value=value,
                attn_mask=attn_mask,
                dropout_p=dropout_p,
                is_causal=is_causal,
                scale=scale,
            )
        return hidden_states

    torch.nn.functional.scaled_dot_product_attention = sdpa_hijack_flash


def patch_attn(attention: AttentionPatch) -> None:
    match attention:
        case AttentionPatch.NONE:
            pass

        case AttentionPatch.Flash:
            try:
                from flash_attn import flash_attn_func  # type: ignore # Missing import

                def sdpa_hijack_flash(q, k, v, m, p, c, s):  # noqa: ANN001, ANN202
                    assert m is None
                    result = flash_attn_func(
                        q=q.transpose(1, 2),
                        k=k.transpose(1, 2),
                        v=v.transpose(1, 2),
                        dropout_p=p,
                        softmax_scale=s if s else q.shape[-1] ** (-0.5),
                        causal=c,
                    )
                    assert isinstance(result, Tensor)
                    return result.transpose(1, 2)

                _patch_sdpa(sdpa_hijack_flash)
            except ImportError:
                LOGQD.exception("Could not load Flash Attention for hijack")

        case AttentionPatch.Sage:
            try:
                from sageattention import sageattn

                def sdpa_hijack_sage(q, k, v, m, p, c, s):  # noqa: ANN001, ANN202
                    assert m is None
                    assert p == 0.0
                    result = sageattn(q, k, v, is_causal=c, sm_scale=s)
                    return result

                _patch_sdpa(sdpa_hijack_sage)
            except ImportError:
                LOGQD.exception("Could not load SageAttention for hijack")

        case _:
            raise NotImplementedError


def apply_loras(loras: list[str], pipe: DiffusionPipeline) -> None:
    adapters = []
    for n, lora in enumerate(loras):
        path, scale = get_suffix(lora, typing=float, default=1.0)
        if scale == 0.0:
            continue
        name = f"LA{n}"
        pipe.load_lora_weights(path, adapter_name=name)
        adapters.append(
            {
                "name": name,
                "scale": scale,
            }
        )

    if adapters:
        pipe.set_adapters(
            adapter_names=[a["name"] for a in adapters],
            adapter_weights=[a["scale"] for a in adapters],
        )
        pipe.fuse_lora(adapter_names=[a["name"] for a in adapters])
        pipe.unload_lora_weights()


def loras_to_str(loras: list[str]) -> str | None:
    scaled_loras = [
        path if scale == 1.0 else f"{path}:::{scale}"
        for path, scale in (get_suffix(lora, typing=float, default=1.0) for lora in loras)
        if scale != 0.0
    ]
    return "\x1f".join(scaled_loras) if scaled_loras else None


def get_compel(pipe: DiffusionPipeline) -> Compel | None:
    try:
        if hasattr(pipe, "tokenizer") and isinstance(pipe.tokenizer, CLIPTokenizer):
            if hasattr(pipe, "tokenizer_2"):
                if isinstance(pipe.tokenizer_2, CLIPTokenizer) and not hasattr(pipe, "tokenizer_3"):
                    compel = Compel(
                        tokenizer=[pipe.tokenizer, pipe.tokenizer_2],
                        text_encoder=[pipe.text_encoder, pipe.text_encoder_2],
                        returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,
                        requires_pooled=[False, True],
                        truncate_long_prompts=False,
                    )
                else:
                    compel = None
            else:
                compel = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder, truncate_long_prompts=False)
        else:
            compel = None

    except BaseException:
        LOGQD.exception("Rich prompting not available.")
        compel = None

    return compel


def set_vae(pipe: DiffusionPipeline, tile: bool) -> None:
    if tile:
        pipe.vae.enable_tiling()
    else:
        pipe.vae.enable_slicing()
    if pipe.dtype != torch.float16:
        pipe.vae.config.force_upcast = False


def get_scheduler(sampler: Sampler, spacing: Spacing | None, default_scheduler: SchedulerMixin) -> SchedulerMixin:
    assert isinstance(default_scheduler, ConfigMixin)

    sampler_args = {
        "steps_offset": 0,
        "set_alpha_to_one": True,
        "final_sigmas_type": "zero",
    }
    if spacing is not None:
        sampler_args["timestep_spacing"] = spacing

    scheduler, scheduler_args = {
        Sampler.Default: (default_scheduler, {}),
        Sampler.Ddim: (DDIMScheduler, {}),
        Sampler.Ddpm: (DDPMScheduler, {}),
        Sampler.Euler: (EulerDiscreteScheduler, {}),
        Sampler.EulerK: (EulerDiscreteScheduler, {"use_karras_sigmas": True}),
        Sampler.EulerE: (EulerDiscreteScheduler, {"use_exponential_sigmas": True}),
        Sampler.EulerB: (EulerDiscreteScheduler, {"use_beta_sigmas": True}),
        Sampler.EulerF: (FlowMatchEulerDiscreteScheduler, {}),
        Sampler.EulerFK: (FlowMatchEulerDiscreteScheduler, {"use_karras_sigmas": True}),
        Sampler.EulerFE: (FlowMatchEulerDiscreteScheduler, {"use_exponential_sigmas": True}),
        Sampler.EulerFB: (FlowMatchEulerDiscreteScheduler, {"use_beta_sigmas": True}),
        Sampler.EulerA: (EulerAncestralDiscreteScheduler, {}),
        Sampler.Dpm: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "dpmsolver++", "solver_order": 1, "use_karras_sigmas": False},
        ),
        Sampler.DpmK: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "dpmsolver++", "solver_order": 1, "use_karras_sigmas": True},
        ),
        Sampler.SDpm: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "sde-dpmsolver++", "solver_order": 1, "use_karras_sigmas": False},
        ),
        Sampler.SDpmK: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "sde-dpmsolver++", "solver_order": 1, "use_karras_sigmas": True},
        ),
        Sampler.Dpm2: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "dpmsolver++", "solver_order": 2, "use_karras_sigmas": False},
        ),
        Sampler.Dpm2K: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "dpmsolver++", "solver_order": 2, "use_karras_sigmas": True},
        ),
        Sampler.SDpm2: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "sde-dpmsolver++", "solver_order": 2, "use_karras_sigmas": False},
        ),
        Sampler.SDpm2K: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "sde-dpmsolver++", "solver_order": 2, "use_karras_sigmas": True},
        ),
        Sampler.Dpm3: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "dpmsolver++", "solver_order": 3, "use_karras_sigmas": False},
        ),
        Sampler.Dpm3K: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "dpmsolver++", "solver_order": 3, "use_karras_sigmas": True},
        ),
        Sampler.SDpm3: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "sde-dpmsolver++", "solver_order": 3, "use_karras_sigmas": False},
        ),
        Sampler.SDpm3K: (
            DPMSolverMultistepScheduler,
            {"algorithm_type": "sde-dpmsolver++", "solver_order": 3, "use_karras_sigmas": True},
        ),
        Sampler.Unipc: (UniPCMultistepScheduler, {"solver_order": 1, "use_karras_sigmas": False}),
        Sampler.UnipcK: (UniPCMultistepScheduler, {"solver_order": 1, "use_karras_sigmas": True}),
        Sampler.Unipc2: (UniPCMultistepScheduler, {"solver_order": 2, "use_karras_sigmas": False}),
        Sampler.Unipc2K: (UniPCMultistepScheduler, {"solver_order": 2, "use_karras_sigmas": True}),
        Sampler.Unipc3: (UniPCMultistepScheduler, {"solver_order": 3, "use_karras_sigmas": False}),
        Sampler.Unipc3K: (UniPCMultistepScheduler, {"solver_order": 3, "use_karras_sigmas": True}),
    }[sampler]

    return scheduler.from_config(default_scheduler.config, **(sampler_args | scheduler_args))


def get_latent_params(pipe: DiffusionPipeline) -> tuple[int, float, int] | None:
    factor: float | None = None
    channels: int | None = None
    default_size: int | None = None

    # The packing causes the config to report (64, 16, 64) or something
    if type(pipe).__name__.startswith("Flux"):
        return (16, 8, 128)
    elif hasattr(pipe, "_unpack_latents"):
        return None

    if hasattr(pipe, "vae_scale_factor"):
        factor = pipe.vae_scale_factor
    if hasattr(pipe, "unet"):
        channels = pipe.unet.config.get("in_channels", None)
        default_size = pipe.unet.config.get("sample_size", None)
    if hasattr(pipe, "transformer"):
        channels = pipe.transformer.config.get("in_channels", None)
        default_size = pipe.transformer.config.get("sample_size", None)
    if hasattr(pipe, "prior_pipe"):
        factor = pipe.prior_pipe.config.get("resolution_multiple", None)
        channels = pipe.prior_pipe.prior.config.get("in_channels", None)

    if factor is not None and channels is not None:
        return (channels, factor, default_size if default_size is not None else round(1024 / factor))
    else:
        return None


# MID LEVEL


def build_jobs(parameters: Parameters) -> list[dict[str, Any]]:
    job = {
        "num_images_per_prompt": parameters.batch_size.value,
        "clean_caption": False,  # stop IF nag. what does this even do
        "use_resolution_binning": False,  # True breaks with pre-noised latents
    }

    if not parameters.negative.value:
        parameters.negative.value = [""]

    image_ops = [{}]

    if parameters.iter.value is Iter.Shuffle:
        jobs = [job] * parameters.batch_count.value_single
        merger = lambda sets, name, vals: [i | {name: v} for i, v in zip(sets, oversample(vals, len(sets)))]  # noqa: E731
    else:
        jobs = [job]
        merger = lambda sets, name, vals: [i | {name: v} for v in vals for i in sets]  # noqa: E731

    for param in parameters.params():
        if param.multi:
            match param.name:
                case "seed" | "lora" | "model" | "dtype" | "sdpb" | "skrample_modifier":
                    pass
                case "power" | "pixelate" | "posterize":
                    if param.value:
                        image_ops = [i | {param.name: v} for i in image_ops for v in param.value]
                case _:
                    if param.value:
                        jobs = merger(jobs, param.name, param.value)

    # manually ordered merges
    if parameters.skrample_modifier.value_multi:
        jobs = merger(jobs, "skrample_modifier", splitlist(parameters.skrample_modifier.value_multi))
    if parameters.lora.value_multi:
        jobs = merger(jobs, "lora", splitlist(parameters.lora.value_multi))
    if parameters.model.value:
        jobs = merger(jobs, "model", parameters.model.value)
    if parameters.dtype.value:
        jobs = merger(jobs, "dtype", parameters.dtype.value)

    i32max = 2**31 - 1
    seeds = (
        [torch.randint(high=i32max, low=-i32max, size=(1,)).item()]
        if not parameters.seed.value
        else parameters.seed.value_multi
    )
    seeds = [
        s + c * parameters.batch_size.value_single for c in range(parameters.batch_count.value_single) for s in seeds
    ]
    match parameters.iter.value:
        case Iter.Shuffle:
            jobs = merger(jobs, "seed", seeds)
        case Iter.Walk:
            # inverse merge so seeds always iter first
            jobs = [
                j | {"seed": s + (n * parameters.batch_size.value_single * parameters.batch_count.value_single)}
                for n, j in enumerate(jobs)
                for s in seeds
            ]
        case Iter.Basic:
            # inverse merge so seeds always iter first
            jobs = [j | {"seed": s} for j in jobs for s in seeds]
        case _:
            raise ValueError

    if parameters.iter.value is Iter.Shuffle:
        jobs = [j | {"image_ops": [o]} for j, o in zip(jobs, oversample(image_ops, len(jobs)))]
    else:
        # Shouldn't need to copy since image_ops isn't mutated
        jobs = [j | {"image_ops": image_ops} for j in jobs]

    for j in jobs:
        for key in "prompt", "negative":
            if key in j:
                expands = pexpand(j[key], body=("[", "]"), single=True)
                assert len(expands) == 1
                j[key] = expands[0]

    return jobs


def get_pipe(
    model: str,
    acc: Accelerator,
    dtype: DType,
    offload: Offload,
    loras: None | list[str],
    img2img: bool,
    tile_vae: bool,
    pag: bool,
) -> DiffusionPipeline:
    pipe_args = {
        "add_watermarker": False,
        "safety_checker": None,
        "torch_dtype": dtype.torch_dtype,
        "use_safetensors": True,
        "watermarker": None,
    }

    if "Kolors" in model:
        pipe_args["variant"] = "fp16"
    elif ("stable-cascade" in model.casefold() or "flux.1" in model.casefold()) and pipe_args[
        "torch_dtype"
    ] == torch.float16:
        pipe_args["torch_dtype"] = torch.bfloat16

    model, revision = get_suffix(model)
    if revision is not None:
        pipe_args["revision"] = revision

    if model.casefold().endswith(".safetensors") or model.casefold().endswith(".sft"):
        pipes: list[type[FromSingleFileMixin]] = [
            # FluxPipeline, # Fucking diffusers won't fetch components
            StableDiffusionXLPipeline,
            StableDiffusionPipeline,
            StableDiffusion3Pipeline,
        ]

        pipe: DiffusionPipeline | None = None
        for cls in pipes:
            assert issubclass(cls, DiffusionPipeline)  # my life for type intersection

            maybe_pipe: DiffusionPipeline | None = None
            try:
                maybe_pipe = cls.from_single_file(model, **pipe_args)
            except SingleFileComponentError as e:
                # better way?
                if str(e).startswith("Failed to load T5EncoderModel.") and issubclass(cls, StableDiffusion3Pipeline):
                    del e  # free gc
                    maybe_pipe = cls.from_single_file(model, text_encoder_3=None, tokenizer_3=None, **pipe_args)
                    break

            if maybe_pipe is not None:
                if hasattr(maybe_pipe, "text_encoder_2") and maybe_pipe.text_encoder_2 is None:
                    continue
                pipe = maybe_pipe
                break

        if pipe is None:
            msg = f'Could not load "{model}" as single file pipeline'
            raise ValueError(msg)
    else:
        pipe = AutoPipelineForText2Image.from_pretrained(model, **pipe_args)

    if img2img:
        pipe = AutoPipelineForImage2Image.from_pipe(pipe, torch_dtype=None)  # avoid recasts

    if pag:
        try:
            if img2img:
                pipe = AutoPipelineForImage2Image.from_pipe(pipe, enable_pag=True, torch_dtype=None)
            else:
                pipe = AutoPipelineForText2Image.from_pipe(pipe, enable_pag=True, torch_dtype=None)
        except BaseException:
            LOGQD.exception(
                f"Could not find a PAG pipeline variant for `{type(pipe).__name__}, parameter `pag` will be ignored"
            )

    assert pipe is not None

    pipe.safety_checker = None
    pipe.watermarker = None

    if hasattr(pipe, "vae"):
        set_vae(pipe, tile_vae)

    if loras is not None:
        apply_loras(loras, pipe)

    if hasattr(pipe, "unet"):
        pipe.unet = acc.prepare_model(pipe.unet)
    if hasattr(pipe, "transformer"):
        pipe.transformer = acc.prepare_model(pipe.transformer)
    if hasattr(pipe, "prior_pipe"):
        pipe.prior_pipe.prior = acc.prepare_model(pipe.prior_pipe.prior)
    if hasattr(pipe, "decoder_pipe"):
        pipe.decoder_pipe.decoder = acc.prepare_model(pipe.decoder_pipe.decoder)

    weight_quant = None
    match dtype:
        case DType.F8:
            # unsigned zero appears much higher quality, closer to i8
            weight_quant = float8_weight_only(torch.float8_e4m3fnuz)
        case DType.F8D:
            weight_quant = float8_dynamic_activation_float8_weight()
        case DType.F6:
            weight_quant = fpx_weight_only(3, 2)
        case DType.I8:
            weight_quant = int8_weight_only()
        case DType.I8D:
            weight_quant = int8_dynamic_activation_int8_weight()
        case DType.I4:
            weight_quant = int4_weight_only()
        case DType.I4D:
            weight_quant = int8_dynamic_activation_int4_weight()
        # Just defaults...
        # GS=128 doesn't work? <64 GS is better spent on more bits
        # HQQ adds too much mem, better spent on more bits
        case DType.U7:
            weight_quant = uintx_weight_only(torch.uint7, 32)  # type: ignore # torch uint
        case DType.U6:
            weight_quant = uintx_weight_only(torch.uint6, 32)  # type: ignore # torch uint
        case DType.U5:
            weight_quant = uintx_weight_only(torch.uint5, 32)  # type: ignore # torch uint
        case DType.U4:
            weight_quant = uintx_weight_only(torch.uint4, 32)  # type: ignore # torch uint
        case DType.U3:
            weight_quant = uintx_weight_only(torch.uint3, 32)  # type: ignore # torch uint
        case DType.U2:
            weight_quant = uintx_weight_only(torch.uint2, 32)  # type: ignore # torch uint
        case DType.U1:
            weight_quant = uintx_weight_only(torch.uint1, 32)  # type: ignore # torch uint

    if weight_quant is not None:
        if offload == Offload.NONE:
            quantize_device = acc.device
        else:
            quantize_device = None
        if hasattr(pipe, "unet"):
            quantize_(pipe.unet, weight_quant, device=quantize_device)
        if hasattr(pipe, "transformer"):
            quantize_(pipe.transformer, weight_quant, device=quantize_device)
        if hasattr(pipe, "prior_pipe"):
            quantize_(pipe.prior_pipe.prior, weight_quant, device=quantize_device)
        if hasattr(pipe, "decoder_pipe"):
            quantize_(pipe.decoder_pipe.decoder, weight_quant, device=quantize_device)
        # It's not worth quantizing CLIP
        if isinstance(getattr(pipe, "text_encoder", None), T5EncoderModel):
            quantize_(pipe.text_encoder, weight_quant, device=quantize_device)
        if isinstance(getattr(pipe, "text_encoder_2", None), T5EncoderModel):
            quantize_(pipe.text_encoder_2, weight_quant, device=quantize_device)
        if isinstance(getattr(pipe, "text_encoder_3", None), T5EncoderModel):
            quantize_(pipe.text_encoder_3, weight_quant, device=quantize_device)

    match offload:
        case Offload.NONE:
            if hasattr(pipe, "prior_pipe"):
                pipe.prior_pipe = pipe.prior_pipe.to(acc.device)
            if hasattr(pipe, "decoder_pipe"):
                pipe.decoder_pipe = pipe.decoder_pipe.to(acc.device)
            pipe = pipe.to(acc.device)
        case Offload.Model:
            pipe.enable_model_cpu_offload()
        case Offload.Sequential:
            pipe.enable_sequential_cpu_offload()
        case _:
            raise ValueError

    return pipe


# Does not use Parameters because effectively all of these are per-job
def make_noise(
    pipe: DiffusionPipeline,
    generators: list[torch.Generator],
    acc: Accelerator,
    # Parameters
    batch_size: int,
    color: LatentColor | None,
    color_power: float,
    noise_power: float,
    noise_type: NoiseType,
    resolution: Resolution | None,
    variance_power: float,
    variance_scale: int,
) -> tuple[torch.Tensor | None, Resolution | None]:
    latent_params = get_latent_params(pipe)
    if latent_params is None:
        return (None, None)

    channels, factor, default_size = latent_params
    if resolution is not None:
        width, height = round(resolution.width / factor), round(resolution.height / factor)
    else:
        width, height = default_size, default_size
    shape = (batch_size, channels, height, width)
    latents = torch.zeros(shape, dtype=pipe.dtype, device="cpu")
    for ln, generator in zip(range(len(latents)), generators):
        # Variance
        if variance_power != 0 and variance_scale > 0:
            # save state so init noise seeds are the same with/without
            state = generator.get_state()
            variance = torch.randn(
                [1, shape[1], variance_scale, variance_scale],
                generator=generator,
                dtype=noise_type.torch_dtype,
                device=noise_type.torch_device or acc.device,
            )
            latents[ln] += torch.nn.UpsamplingBilinear2d((shape[2], shape[3]))(variance).mul(variance_power)[0].cpu()
            generator.set_state(state)
        # Init noise
        noise = torch.randn(
            latents.shape[1:],
            generator=generator,
            dtype=noise_type.torch_dtype,
            device=noise_type.torch_device or acc.device,
        )
        if noise_power != 1:
            noise *= noise_power
        latents[ln] += noise.cpu()

    # Colored latents
    if color is not None and color_power != 0:
        if is_xl_vae(pipe):
            cols = COLS_XL
        elif is_sd_vae(pipe):
            cols = COLS_FTMSE
        elif is_sd3_vae(pipe):
            cols = COLS_SD3
        elif is_flux_vae(pipe):
            cols = COLS_FLUX
        else:
            cols = None
            LOGQD.error(f"Parameter `color_power` cannot be used for {type(pipe).__name__}")
        if cols:
            euler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)
            assert isinstance(euler, EulerDiscreteScheduler)
            latents += (
                torch.tensor(cols[color], dtype=pipe.dtype, device="cpu")
                .mul(color_power)
                .div(euler.init_noise_sigma)
                .expand([shape[0], shape[2], shape[3], shape[1]])
                .permute((0, 3, 1, 2))
            )

    # Why does the pipe not do this...
    if type(pipe).__name__.startswith("Flux"):
        b, c, h, w = latents.shape
        latents = latents.view(b, c, h // 2, 2, w // 2, 2)
        latents = latents.permute(0, 2, 4, 1, 3, 5)
        latents = latents.reshape(b, (h // 2) * (w // 2), c * 4)

    return (latents, Resolution((round(width * factor), round(height * factor))))


# TOP LEVEL


@torch.no_grad()  # torchao breaks with inference_mode
def process_job(
    parameters: Parameters,
    piperef: PipeRef,
    acc: Accelerator,
    job: dict[str, Any],
    meta: dict[str, str],
    input_image: Image.Image | None,
) -> list[tuple[dict[str, Any], Image.Image, PngImagePlugin.PngInfo]]:
    # POP SEED first because the meta is set in PngInfo directly
    seed = job.pop("seed")

    # LOAD COMMON META
    for param in parameters.params():
        if param.name in job and param.meta and param.name != "lora":
            meta[param.name] = job[param.name]

    loras = job.pop("lora", [])
    lora_meta = loras_to_str(loras)
    if lora_meta is not None:
        meta["lora"] = lora_meta

    # POP PARAMS
    model = job.pop("model")
    model_dtype = job.pop("dtype", DType.F16)
    color = job.pop("color", None)
    color_power = job.pop("color_power", 0)
    noise_power = job.pop("noise_power", 1)
    noise_type = job.pop("noise_type", NoiseType.Cpu32)
    resolution: Resolution | None = job.pop("resolution", None)
    variance_power = job.pop("variance_power", 0)
    variance_scale = job.pop("variance_scale", 0)
    image_ops: list[dict[str, Any]] = job.pop("image_ops")
    sampler: Sampler = job.pop("sampler", Sampler.Default)
    sksampler: SamplerSK = job.pop("skrample_sampler", SamplerSK.NONE)
    skschedule: ScheduleSK = job.pop("skrample_schedule", ScheduleSK.Default)
    skmodifier: list[str] = job.pop("skrample_modifier", [])
    skmodmerge: MergeStrategy = job.pop("skrample_modifier_merge", MergeStrategy.UniqueBefore)
    sknoise: NoiseSK = job.pop("skrample_noise", NoiseSK.Random)
    skpredictor: PredictorSK = job.pop("skrample_predictor", PredictorSK.Default)
    skdtype: DTypeSK = job.pop("skrample_dtype", DTypeSK.F64)

    if skmodifier:
        meta["skrample_modifier"] = "\x1f".join(skmodifier)

    # PIPE
    if piperef.name != model or piperef.loras != lora_meta or piperef.dtype != model_dtype:
        if piperef.pipe is not None:
            piperef.pipe.to("meta")
        del piperef.pipe
        piperef.pipe = get_pipe(
            model,
            acc,
            model_dtype,
            parameters.offload.value_single,
            loras,
            input_image is not None,
            parameters.tile.value_single,
            any(parameters.pag.value_multi),
        )
        piperef.name = model
        piperef.loras = lora_meta
        piperef.dtype = model_dtype
    pipe = piperef.pipe
    if pipe is None:
        return []
    assert isinstance(pipe, Callable)

    aot: str | None = None
    if "Sana" in pipe.__class__.__name__:  # Sana is too touchy
        aot = os.environ.get("TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL", "0")
        os.environ["TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL"] = "0"

    # INPUT TENSOR
    generators = [
        torch.Generator(noise_type.torch_device or acc.device).manual_seed(seed + n)
        for n in range(parameters.batch_size.value_single)
    ]
    if input_image is None:
        latents, latents_resolution = make_noise(
            pipe,
            generators,
            acc,
            # Parameters
            parameters.batch_size.value_single,
            color,
            color_power,
            noise_power,
            noise_type,
            resolution,
            variance_power,
            variance_scale,
        )
        if latents is not None and latents_resolution is not None:
            job["latents"] = latents
            job["width"], job["height"] = latents_resolution.resolution
        elif resolution is not None:
            job["width"], job["height"] = resolution.resolution
    else:
        if resolution is not None:
            job["image"] = input_image.resize(resolution.resolution, Image.Resampling.LANCZOS)
        else:
            job["image"] = input_image
    job["generator"] = generators
    LOGQD.info(f"seeds: {' '.join([str(seed + n) for n in range(parameters.batch_size.value_single)])}")

    compel = get_compel(pipe)
    if compel is not None:
        pos = job.pop("prompt") if "prompt" in job else ""
        neg = job.pop("negative") if "negative" in job else ""
        if hasattr(pipe, "tokenizer_2"):
            ncond, npool = compel.build_conditioning_tensor(neg)
            pcond, ppool = compel.build_conditioning_tensor(pos)
            job = job | {"pooled_prompt_embeds": ppool, "negative_pooled_prompt_embeds": npool}
        else:
            pcond = compel.build_conditioning_tensor(pos)
            ncond = compel.build_conditioning_tensor(neg)
        pcond, ncond = compel.pad_conditioning_tensors_to_same_length([pcond, ncond])
        job |= {"prompt_embeds": pcond, "negative_prompt_embeds": ncond}

    pipe_params = signature(pipe).parameters  # type: ignore Callable

    if hasattr(pipe, "scheduler"):
        default_scheduler = pipe.scheduler

        # Set diffusers first so skrample can read it
        pipe.scheduler = get_scheduler(sampler, job.get("spacing", None), pipe.scheduler)

        if sksampler != SamplerSK.NONE:
            sampler_type, sampler_props = sksampler.sampler
            schedule_type, schedule_props = skschedule.schedule
            noise_type, noise_props = sknoise.noise

            if sampler_type is not None and issubclass(sampler_type, HighOrderSampler):
                order = job.get("skrample_order", None)
                if order is not None:
                    if order > sampler_type.max_order():
                        LOGQD.warning(
                            f"Selected order {order} larger than "
                            f"{sampler_type.__name__} max order {sampler_type.max_order()}"
                        )
                    sampler_props["order"] = order

            pipe.scheduler = SkrampleWrapperScheduler.from_diffusers_config(
                pipe.scheduler,  # type: ignore ConfigMixin
                sampler=sampler_type,
                schedule=schedule_type,
                schedule_modifiers=[p for p in (ModifierSK.parse_suffix(i) for i in skmodifier) if p],
                predictor=skpredictor.predictor,
                noise_type=noise_type,
                noise_props=noise_props,
                compute_scale=skdtype.torch_dtype,
                sampler_props=sampler_props,
                schedule_props=schedule_props,
                modifier_merge_strategy=skmodmerge,
                allow_dynamic=not any("shift" in p[1] for p in (ModifierSK.parse_suffix(i) for i in skmodifier) if p),
            )

        else:
            pipe.scheduler = get_scheduler(sampler, job.get("spacing", None), pipe.scheduler)
    else:
        default_scheduler = None

    if "prior_num_inference_steps" in pipe_params and "steps" in job:
        steps = job.pop("steps")
        job["prior_num_inference_steps"] = steps
        if "decoder_steps" in job:
            decoder_steps = job.pop("decoder_steps")
            job["num_inference_steps"] = (
                round(spowf(abs(steps * decoder_steps), 1 / 2)) if decoder_steps < 0 else decoder_steps
            )

    for f, t in [
        ("steps", ["num_inference_steps"]),
        ("denoise", ["strength"]),
        ("negative", ["negative_prompt"]),
        ("guidance", ["guidance_scale", "prior_guidance_scale"]),
        ("decoder_guidance", ["decoder_guidance_scale"]),
        ("rescale", ["guidance_rescale"]),
        ("pag", ["pag_scale"]),
    ]:
        if f in job:
            for to in t:
                job[to] = job[f]
            del job[f]

    # make sure call doesnt err
    for k in list(job.keys()):
        if k not in pipe_params:
            del job[k]

    results: list[tuple[dict[str, Any], Image.Image, PngImagePlugin.PngInfo]] = []
    for n, image_array in enumerate(pipe(output_type="np", **job).images):
        pnginfo = PngImagePlugin.PngInfo()
        for k, v in meta.items():
            pnginfo.add_text(k, str(v))
            pnginfo.add_text("seed", str(seed + n))

        for ops in image_ops:
            info = copy(pnginfo)
            for k, v in ops.items():
                info.add_text(k, str(v))

            # Direct array ops
            op_arr: np.ndarray = np.asarray(image_array)  # mutable reference to make pyright happy

            if "power" in ops:
                # ^2.2 for approx sRGB EOTF
                okl = lrgb_to_oklab(spowf_np(op_arr, 2.2))

                # 1/3 cause OKL's top heavy lightness curve
                offset = [0.35, 1, 1]
                # Halve chromacities' power slope
                power = [ops["power"], spowf(ops["power"], 1 / 2), spowf(ops["power"], 1 / 2)]

                okl = spowf_np((okl + offset), power) - offset

                # back to sRGB with approx OETF
                op_arr = spowf_np(oklab_to_lrgb(okl), 1 / 2.2)

            if "posterize" in ops:
                if ops["posterize"] > 1:
                    factor = float((ops["posterize"] - 1) / 256)
                    op_arr = (op_arr * 255 * factor).round() / factor / 255

            # PIL ops
            op_pil: Image.Image = Image.fromarray((op_arr * 255).clip(0, 255).astype(np.uint8))
            del op_arr

            if "pixelate" in ops:
                if ops["pixelate"] > 1:
                    w, h = op_pil.width, op_pil.height
                    op_pil = op_pil.resize(
                        (round(w / ops["pixelate"]), round(h / ops["pixelate"])),
                        resample=Image.Resampling.BOX,
                    )
                    op_pil = op_pil.resize((w, h), resample=Image.Resampling.NEAREST)

            # insert extra non-png meta for grid making
            results.append(
                (
                    meta
                    | ops
                    | {"seed": seed + n, "resolution": op_pil.size, "dtype": model_dtype, "lora": lora_meta or "-"},
                    op_pil,
                    info,
                )
            )

    if default_scheduler is not None:
        pipe.scheduler = default_scheduler

    if aot is not None:
        os.environ["TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL"] = aot

    return results


def main(parameters: Parameters, meta: dict[str, str], image: Image.Image | None) -> None:
    acc = Accelerator(device_placement=False, dynamo_plugin=parameters.compile.value_single.plugin)
    images: list[tuple[dict[str, Any], Image.Image, PngImagePlugin.PngInfo]] = []
    piperef = PipeRef()
    jobs = build_jobs(parameters)

    if parameters.tunable.value and torch.cuda.is_available():
        torch.cuda.tunable.enable(val=True)

    total_images = len(jobs) * parameters.batch_size.value_single
    LOGQD.info(f"Generating {len(jobs)} batches of {parameters.batch_size.value} images for {total_images} total")
    pbar = tqdm(desc="Images", total=total_images, smoothing=0)

    if parameters.attn_patch.value_single is not AttentionPatch.NONE:
        patch_attn(parameters.attn_patch.value_single)

    kernel_ctx = nullcontext()
    if parameters.sdpb.value_multi:
        kernel_ctx = torch.nn.attention.sdpa_kernel([k.torch_sdp_backend for k in parameters.sdpb.value_multi])

    with kernel_ctx, concurrent.futures.ThreadPoolExecutor() as tpe:
        im_num = 0
        for job in jobs:
            with SmartSigint(job_name="current batch"):
                results = process_job(parameters, piperef, acc, job, meta.copy(), image)

                if parameters.grid.value is not None:
                    images += results

                for _, im, info in results:
                    im_path = parameters.output.value_single.joinpath(f"{im_num:05}.png")
                    while im_path.exists():
                        im_num += 1
                        im_path = parameters.output.value_single.joinpath(f"{im_num:05}.png")
                    tpe.submit(Image.Image.save, im, im_path, "PNG", pnginfo=info, compress_level=9)
                    im_num += 1

            pbar.update(parameters.batch_size.value_single)

        if parameters.grid.value is not None:
            gd_num = 0
            grids, others = parameters.grid.value_single.fold((m, i) for m, i, _ in images)
            for gd in grids:
                gd_path = parameters.output.value_single.joinpath(f"grid_{gd_num:05}.png")
                while gd_path.exists():
                    gd_num += 1
                    gd_path = parameters.output.value_single.joinpath(f"grid_{gd_num:05}.png")
                tpe.submit(Image.Image.save, gd, gd_path, "PNG", compress_level=4)
                gd_num += 1

            if len(others) > 0:
                LOGQD.warning(f"{len(others)} images not included in grid output")


if __name__ == "__main__":
    main_parameters = locals()["cli_params"]
    main_image = locals()["cli_image"]
    main_meta: dict[str, str] = {"url": "https://github.com/Beinsezii/quickdif"}
    if main_parameters.comment.value:
        main_meta["comment"] = main_meta["comment"] = main_parameters.comment.value

    main(main_parameters, main_meta, main_image)
